<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shi (Billy) Chen</title>

    <meta name="author" content="Shi (Billy) Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="images/favicon/favicon.ico">
    <link rel="icon" type="image/svg+xml" href="images/favicon/favicon.svg">
    <link rel="icon" type="image/png" sizes="96x96" href="images/favicon/favicon-96x96.png">
    <link rel="apple-touch-icon" sizes="180x180" href="images/favicon/apple-touch-icon.png">
    <link rel="manifest" href="images/favicon/site.webmanifest">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shi (Billy) Chen
                </p>
                <p>
                I'm a final-year undergraduate at <a href="https://www.fudan.edu.cn/en/">Fudan University</a> studying Computer Science and Technology, and an exchange student at <a href="https://www.berkeley.edu/">UC Berkeley</a>.
                I'm the Co-founder & CTO of <a href="https://intuition.dev/">Intuition Core Inc.</a>, an AI & Robotics startup developing infrastructure solutions to accelerate real-world robot deployment.
                </p>
                <p>
                My research interests span <strong>computer vision</strong>, <strong>3D generation</strong>, <strong>robotics</strong>, and <strong>embodied AI</strong>. I've been fortunate to work with amazing researchers at UC Berkeley, University of Chicago, University of Cambridge, and Johns Hopkins University.
                I'm honored to be funded by the <a href="https://www.academicenter.com/news/details/1838873467108659200.html">National Science Foundation of China under its Youth Fund</a> (one of the highest honors for undergraduates in China; ~120 students nationwide annually).
                </p>
                <p style="text-align:center">
                  <a href="mailto:shichen0122@163.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/ShiChen-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://github.com/BillyChern">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/ShiChen.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/ShiChen.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <!-- News Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li><strong>[May 2025]</strong> Co-founded Intuition Core Inc. and secured $950K in pre-seed funding from Founders Fund and Fen Venture at $24M post-money valuation!</li>
                  <li><strong>[Jan 2025]</strong> Started exchange program at UC Berkeley, taking courses on Programming Languages and Compilers, Robotic Manipulation and Interaction, and Advanced Large Language Model Agents.</li>
                  <li><strong>[Aug 2024]</strong> Completed summer research at University of Cambridge, graduating with First with Distinction.</li>
                  <li><strong>[Sept 2024]</strong> Awarded funding by the National Science Foundation of China under its Youth Fund.</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <!-- Entrepreneurial Experience -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Entrepreneurial Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle;text-align:center">
                <a href="https://intuition.dev/">
                  <img src='images/intuition_logo_round.png' width=100% style="max-width:240px;">
                </a>
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://intuition.dev/">
                  <span class="papertitle">Intuition Core Inc.</span>
                </a>
                <br>
                <strong>Co-founder & CTO</strong>, May 2025 - Present
                <br>
                <em>Berkeley & San Francisco, CA</em>
                <p></p>
                <p>
                Co-founded an AI & Robotics startup developing infrastructure solutions to accelerate real-world robot deployment. Designed a novel Masked Autoencoders (MAE) based world model pretraining architecture achieving 15%+ downstream policy training efficiency. Built ultra-low latency (&lt;100ms) teleoperation systems over long distances (Hawaii-SF, Shanghai-SF). Secured $950K in pre-seed funding from Founders Fund and Fen Venture at $24M post-money valuation cap.
                </p>
                <p style="margin-top:8px;margin-bottom:4px;font-size:small;font-style:italic;">
                Demo video of our Vision-Language-Action (VLA) model trained with our customized data collection pipeline and optimized with our world model pre-training method:
                </p>
                <video width="100%" autoplay loop muted playsinline controls style="border-radius:4px;">
                  <source src="images/demo_vla.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </td>
            </tr>

          </tbody></table>

          <!-- Research -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, 3D generation, robotics, and embodied AI. My research focuses on developing methods that enable machines to perceive, understand, and interact with the physical world. Representative works are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!-- LiDAR Research -->
            <tr onmouseout="lidar_stop()" onmouseover="lidar_start()" style="background-color: #ffffd0;">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='lidar_image'>
                    <video width="100%" height="100%" autoplay loop muted playsinline>
                      <source src="images/lidar_before.mp4" type="video/mp4">
                    </video>
                  </div>
                  <video width="100%" height="100%" autoplay loop muted playsinline>
                    <source src="images/lidar_after.mp4" type="video/mp4">
                  </video>
                </div>
                <script type="text/javascript">
                  function lidar_start() {
                    document.getElementById('lidar_image').style.opacity = "1";
                  }
                  function lidar_stop() {
                    document.getElementById('lidar_image').style.opacity = "0";
                  }
                  lidar_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Semantic Segmentation and Completion for LiDAR Signals</span>
                <br>
                <strong>Shi Chen</strong>, Weifeng Ge
                <br>
                Advisor: <a href="https://www.weifengge.net/">Prof. Weifeng Ge</a>, Fudan University
                <br>
                <em>In preparation for IEEE TPAMI</em>, Sept. 2024 - Present
                <br>
                <a href="#">project page (coming soon)</a>
                <p></p>
                <p>
                Proposed a 3D latent diffusion model with multi-stage data augmentation for sparse LiDAR datasets. Designed a multinomial discrete diffusion model achieving 39 mIoU, surpassing previous SOTA (37.9 mIoU). Funded by the <a href="https://www.academicenter.com/news/details/1838873467108659200.html">National Science Foundation of China Youth Fund</a>. Currently deploying on Unitree G1 humanoid and Go1 robot dog for outdoor navigation.
                </p>
              </td>
            </tr>

            <!-- Mesh Optimization Research -->
            <tr onmouseout="mesh_stop()" onmouseover="mesh_start()" style="background-color: #ffffd0;">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mesh_image'>
                    <video width="100%" height="100%" autoplay loop muted playsinline>
                      <source src="images/mesh_before.mp4" type="video/mp4">
                    </video>
                  </div>
                  <video width="100%" height="100%" autoplay loop muted playsinline>
                    <source src="images/mesh_after.mp4" type="video/mp4">
                  </video>
                </div>
                <script type="text/javascript">
                  function mesh_start() {
                    document.getElementById('mesh_image').style.opacity = "1";
                  }
                  function mesh_stop() {
                    document.getElementById('mesh_image').style.opacity = "0";
                  }
                  mesh_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Mesh & Texture Optimization for 3D Generation</span>
                <br>
                <strong>Shi Chen</strong>, Rana Hanocka
                <br>
                Advisor: <a href="https://people.cs.uchicago.edu/~ranahanocka/">Prof. Rana Hanocka</a>, University of Chicago
                <br>
                <em>Individual Summer Research</em>, June - Aug. 2025
                <br>
                <a href="#">project page (coming soon)</a>
                <p></p>
                <p>
                Extended the Continuous Remeshing pipeline to generate high-quality meshes from single-frame images. Introduced vertex color optimization with novel loss terms. Demonstrated improved mesh quality over baseline Marching Cube algorithm with >70% preference in crowdsourced user study.
                </p>
              </td>
            </tr>

            <!-- Quadrotor Research -->
            <tr onmouseout="drone_stop()" onmouseover="drone_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='drone_image'>
                    <img src='images/drone_after.png' width=100%>
                  </div>
                  <video width="100%" height="100%" autoplay loop muted playsinline>
                    <source src="images/drone_before.mp4" type="video/mp4">
                  </video>
                </div>
                <script type="text/javascript">
                  function drone_start() {
                    document.getElementById('drone_image').style.opacity = "1";
                  }
                  function drone_stop() {
                    document.getElementById('drone_image').style.opacity = "0";
                  }
                  drone_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Flying in the Wild, No GPS</span>
                <br>
                <strong>Shi Chen</strong>, Francesco Crivelli, Peiqi Liu, Siddharth Nath
                <br>
                Advisor: <a href="https://people.eecs.berkeley.edu/~sastry/">Prof. Shankar Sastry</a>, UC Berkeley
                <br>
                <em>Research Project</em>, Apr. - June 2025
                <br>
                <a href="https://drive.google.com/file/d/1SJAF8D8OEFBW1JrRByaXCItS4d7UCxGC/view?usp=drive_link">paper</a>
                <p></p>
                <p>
                Built a custom quadrotor under the Agilicious framework with specialized hardware. Enabled real-time state estimation using SVO Pro algorithm for GPS-free navigation. Fine-tuned Qwen-2.5 3B VLM for onboard deployment on Jetson Orin Nano. Verified feasibility of onboard VLM deployment for real-time quadrotor navigation, reducing hardware costs by several hundred dollars.
                </p>
              </td>
            </tr>

            <!-- Hand Generation Research -->
            <tr style="background-color: #ffffd0;">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/hand.png' width=100%>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Hi-Fi: High-Quality Synthetic Hand-Over-Face Gestures Dataset with Multimodal Diffusion</span>
                <br>
                <strong>Shi Chen</strong>, Marwa Mahmoud
                <br>
                Advisor: <a href="https://www.cl.cam.ac.uk/~mmam3/">Prof. Marwa Mahmoud</a>, University of Cambridge
                <br>
                <em>IEEE International Conference on Automatic Face and Gesture Recognition (FG2025)</em>, July - Aug. 2024
                <br>
                <a href="https://drive.google.com/file/d/1zeovUEiXmXjnyF5aEbghHt13j-CD0Mux/view?usp=drive_link">paper</a>
                <p></p>
                <p>
                Proposed a multimodal diffusion pipeline integrating ControlNet into Stable Diffusion, increasing MediaPipe Confidence scores from 0.248 to 0.556 (2.24x improvement). Created a synthetic dataset of 170,000+ images across 809 gesture types with 83.5% quality rating. Selected for Cambridge Summer Research Programme, graduating with First with Distinction.
                </p>
              </td>
            </tr>

            <!-- Autonomous Driving Research -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/av.png' width=100%>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Reconstructing Streets and Augmenting Autonomous Driving</span>
                <br>
                <strong>Shi Chen</strong>, Xingrui Wang
                <br>
                Collaborator: <a href="https://xingruiwang.github.io/">Dr. Xingrui Wang</a>, Johns Hopkins University
                <br>
                <em>Research Project</em>, Jan. - July 2024
                <p></p>
                <p>
                Developed a diffusion model to generate 3D bounding boxes and BEV perception data on NuScenes dataset. Combined synthetic input for MagicDrive to create temporally consistent RGB videos. Achieved 2-4% accuracy improvement on downstream tasks including BEV perception and 3D bounding box detection on BEVFusion. Used NeRF-based Nerfacto for controllable 3D scene rendering.
                </p>
              </td>
            </tr>

          </tbody></table>

          <!-- Education -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="https://www.berkeley.edu/">
                  <img src='images/Seal_of_University_of_California,_Berkeley.svg' width=100%>
                </a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://www.berkeley.edu/">
                  <span class="papertitle">University of California, Berkeley</span>
                </a>
                <br>
                <strong>Exchange Student</strong>, Jan. - Aug. 2025
                <br>
                <em>GPA: 3.91/4.0</em>
                <br>
                Courses: Programming Languages and Compilers, Robotic Manipulation and Interaction, Advanced Large Language Model Agents
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href="https://www.fudan.edu.cn/en/">
                  <img src='images/Fudan_University_Logo.svg' width=100%>
                </a>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://www.fudan.edu.cn/en/">
                  <span class="papertitle">Fudan University</span>
                </a>
                <br>
                <strong>B.Eng. Computer Science and Technology</strong>, Sept. 2022 - July 2026 (expected)
                <br>
                <em>GPA: 93/100 (Major GPA: 3.80/4.0); Ranking: 11/91</em>
                <br>
                Supervised by <a href="https://www.weifengge.net/">Prof. Weifeng Ge</a>
                <br>
                <strong>Honors:</strong> National Science Foundation of China Youth Fund (2024), Fudan University Scholarship for Academic Achievement: First with Distinction (2024-2025), Second Prize in China Undergraduate Mathematical Contest in Modeling (2023), Third Prize in Chinese Undergraduate Mathematics Competitions (2023)
              </td>
            </tr>

          </tbody></table>

          <!-- Other Selected Projects -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Other Selected Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/stateful_agent.jpg' width=100%>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Stateful-Agent: A Cross-Platform LLM-Based Agent with Persistent Memory</span>
                <br>
                <strong>Shi Chen</strong> and collaborators
                <br>
                Course Instructor: <a href="https://people.eecs.berkeley.edu/~dawnsong/">Prof. Dawn Song</a>, UC Berkeley
                <br>
                <em>Course Project</em>, Apr. - June 2025
                <br>
                <a href="https://github.com/BillyChern/stateful-agent">GitHub</a>
                /
                <a href="https://github.com/nsd9696/stateful-agent-chrome-extension">Chrome Extension</a>
                <p></p>
                <p>
                Developed a stateful LLM agent with persistent memory for academic research management. Built Chrome extension to auto-fill graduate school applications. Integrated automated paper discovery using LangChain + GPT-4o. Achieved 98% success rate in automated posting tasks.
                </p>
              </td>
            </tr>

          </tbody></table>

          <!-- Skills & Technical Expertise -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Skills & Technical Expertise</h2>
                <p>
                  <strong>Programming Languages:</strong> C, C++, Python, OCaml, Lean (Theorem Proving), Verilog HDL, Pascal, MATLAB
                  <br>
                  <strong>Frameworks:</strong> PyTorch, PyTorch3d, JAX, OpenCV, NumPy, LangChain
                  <br>
                  <strong>Robotics & Simulation:</strong> ROS / ROS2, MuJoCo, OpenAI Gym
                  <br>
                  <strong>Modeling & Design:</strong> Blender (3D Modeling), Unity
                  <br>
                  <strong>Tools:</strong> Git, Linux, Docker, LaTeX
                  <br>
                  <strong>Languages:</strong> English (TOEFL 106, Speaking 25; GRE 336, Quantitative 170)
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Footer -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
